{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "80bc9016-977d-44e2-83a6-b37316d58171",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "from tensorflow.keras import layers\n",
    "import matplotlib\n",
    "import matplotlib.pyplot as plt\n",
    "import gym\n",
    "import gym_platoon\n",
    "import sc"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a21342e3-4a7a-4d6d-a5a7-f98e7c1d1848",
   "metadata": {},
   "source": [
    "# From DQN tutorial on Keras"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "ce75a482-511a-4e1a-a08f-cf51670b22e4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Configuration paramaters for the whole setup\n",
    "seed = 42\n",
    "gamma = 0.8  # Discount factor for past rewards\n",
    "epsilon = 1.0  # Epsilon greedy parameter\n",
    "epsilon_min = 0.1  # Minimum epsilon greedy parameter\n",
    "epsilon_max = 1.0  # Maximum epsilon greedy parameter\n",
    "epsilon_interval = (\n",
    "    epsilon_max - epsilon_min\n",
    ")  # Rate at which to reduce chance of random action being taken\n",
    "batch_size = 32  # Size of batch taken from replay buffer\n",
    "max_steps_per_episode = 20"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "353d9da4-cfef-46e5-a6ca-644c3b3d1709",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[ 0 10  0  0]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/srahmoun/opt/anaconda3/envs/rlsc/lib/python3.7/site-packages/gym/spaces/box.py:74: UserWarning: \u001b[33mWARN: Box bound precision lowered by casting to float32\u001b[0m\n",
      "  \"Box bound precision lowered by casting to {}\".format(self.dtype)\n"
     ]
    }
   ],
   "source": [
    "# Platooning Environment defined in gym-platoon\n",
    "env = gym.make('platoon-v1')\n",
    "env.seed(seed)\n",
    "num_states = env.observation_space.shape[0]\n",
    "print(env.reset())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "eac902ed-efce-416d-9371-a03cbcbcb81b",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2021-12-01 06:19:54.979202: I tensorflow/core/platform/cpu_feature_guard.cc:145] This TensorFlow binary is optimized with Intel(R) MKL-DNN to use the following CPU instructions in performance critical operations:  SSE4.1 SSE4.2\n",
      "To enable them in non-MKL-DNN operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "2021-12-01 06:19:54.980072: I tensorflow/core/common_runtime/process_util.cc:115] Creating new thread pool with default inter op setting: 8. Tune using inter_op_parallelism_threads for best performance.\n"
     ]
    }
   ],
   "source": [
    "num_actions = 9\n",
    "\n",
    "\n",
    "def create_q_model():\n",
    "    \n",
    "    inputs = layers.Input(shape=(num_states,))\n",
    "    layer1 = layers.Dense(256, activation=\"relu\")(inputs)\n",
    "    layer2 = layers.Dense(256, activation=\"relu\")(layer1)\n",
    "    action = layers.Dense(1, activation=\"linear\")(layer2)\n",
    "\n",
    "\n",
    "    return keras.Model(inputs=inputs, outputs=action)\n",
    "\n",
    "\n",
    "# The first model makes the predictions for Q-values which are used to\n",
    "# make a action.\n",
    "model = create_q_model()\n",
    "# Build a target model for the prediction of future rewards.\n",
    "# The weights of a target model get updated every 10000 steps thus when the\n",
    "# loss between the Q-values is calculated the target Q-value is stable.\n",
    "model_target = create_q_model()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "c6102426-fe03-4243-8d72-d122043e1754",
   "metadata": {},
   "outputs": [],
   "source": [
    "def translateAction(action):\n",
    "    if action == 0:\n",
    "        return [1,1]\n",
    "    if action == 1:\n",
    "        return [1,0]\n",
    "    if action == 2: \n",
    "        return [1,-1]\n",
    "    if action == 3:\n",
    "        return [-1, 1]\n",
    "    if action == 4:\n",
    "        return [-1, 0]\n",
    "    if action == 5:\n",
    "        return [-1,-1]\n",
    "    if action == 6:\n",
    "        return [0,-1]\n",
    "    if action == 7:\n",
    "        return [0,1]\n",
    "    if action == 8:\n",
    "        return [0,0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "c4aaae6d-795c-42ed-bf61-2fa7306155a2",
   "metadata": {},
   "outputs": [],
   "source": [
    "def translateState(state):\n",
    "    xl = [state[0], state[1]] \n",
    "    vl = [state[2], state[3]]\n",
    "    return xl, vl"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "d7f363ad-a434-4cc9-b6f2-309c4eb11aff",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "running reward: -190.87 at episode 677, frame count 10000\n"
     ]
    }
   ],
   "source": [
    "# Adam Optimizer w/ Learning rate\n",
    "optimizer = keras.optimizers.Adam(learning_rate=0.00025, clipnorm=1.0)\n",
    "\n",
    "# Experience replay buffers\n",
    "action_history = []\n",
    "state_history = []\n",
    "state_next_history = []\n",
    "rewards_history = []\n",
    "done_history = []\n",
    "episode_reward_history = []\n",
    "running_reward = 0\n",
    "episode_count = 0\n",
    "frame_count = 0\n",
    "# Number of frames to take random action and observe output\n",
    "epsilon_random_frames = 50000\n",
    "# Number of frames for exploration\n",
    "epsilon_greedy_frames = 1000000.0\n",
    "# Maximum replay length\n",
    "max_memory_length = 100000\n",
    "# Train the model after 4 actions\n",
    "update_after_actions = 4\n",
    "# How often to update the target network\n",
    "update_target_network = 10000\n",
    "# Using huber loss for stability\n",
    "loss_function = keras.losses.Huber()\n",
    "# Plotting purposes\n",
    "episodes = []\n",
    "cumulative_reward = []\n",
    "\n",
    "while (episode_count < 1000):  # Run until solved\n",
    "    state = np.array(env.reset())\n",
    "    xl, vl = translateState(state)\n",
    "    cont = sc.Controller(xl, vl)\n",
    "    episode_reward = 0\n",
    "    cumulative_crashes = []\n",
    "    running_crashes = 0\n",
    "\n",
    "    for timestep in range(1, max_steps_per_episode):\n",
    "        # env.render(); Adding this line would show the attempts\n",
    "        # of the agent in a pop up window.\n",
    "        frame_count += 1\n",
    "\n",
    "        # Use epsilon-greedy for exploration\n",
    "        if frame_count < epsilon_random_frames or epsilon > np.random.rand(1)[0]:\n",
    "            # Take random action\n",
    "            action = np.random.choice(num_actions)\n",
    "        else:\n",
    "            # Predict action Q-values\n",
    "            # From environment state\n",
    "            state_tensor = tf.convert_to_tensor(state)\n",
    "            state_tensor = tf.expand_dims(state_tensor, 0)\n",
    "            action_probs = model(state_tensor, training=False)\n",
    "            # Take best action\n",
    "            action = tf.argmax(action_probs[0]).numpy()\n",
    "\n",
    "        if (not cont.isSafe(translateAction(action))):\n",
    "            continue\n",
    "        # Decay probability of taking random action\n",
    "        epsilon -= epsilon_interval / epsilon_greedy_frames\n",
    "        epsilon = max(epsilon, epsilon_min)\n",
    "\n",
    "        # Apply the sampled action in our environment\n",
    "        state_next, reward, done, _ = env.step(action)\n",
    "        state_next = state_next\n",
    "\n",
    "        episode_reward += reward\n",
    "\n",
    "        # Save actions and states in replay buffer\n",
    "        action_history.append(action)\n",
    "        state_history.append(state)\n",
    "        state_next_history.append(state_next)\n",
    "        done_history.append(done)\n",
    "        rewards_history.append(reward)\n",
    "        state = np.array(state_next)\n",
    "        \n",
    "        if state[0] > state[1]: \n",
    "            running_crashes += 1 \n",
    "\n",
    "        # Update every fourth frame and once batch size is over 32\n",
    "        if frame_count % update_after_actions == 0 and len(done_history) > batch_size:\n",
    "\n",
    "            # Get indices of samples for replay buffers\n",
    "            indices = np.random.choice(range(len(done_history)), size=batch_size)\n",
    "\n",
    "            # Using list comprehension to sample from replay buffer\n",
    "            state_sample = np.array([state_history[i] for i in indices])\n",
    "            state_next_sample = np.array([state_next_history[i] for i in indices])\n",
    "            rewards_sample = [rewards_history[i] for i in indices]\n",
    "            action_sample = [action_history[i] for i in indices]\n",
    "            done_sample = tf.convert_to_tensor(\n",
    "                [float(done_history[i]) for i in indices]\n",
    "            )\n",
    "\n",
    "            # Build the updated Q-values for the sampled future states\n",
    "            # Use the target model for stability\n",
    "            future_rewards = model_target.predict(state_next_sample)\n",
    "            # Q value = reward + discount factor * expected future reward\n",
    "            updated_q_values = rewards_sample + gamma * tf.reduce_max(\n",
    "                future_rewards, axis=1\n",
    "            )\n",
    "\n",
    "            # If final frame set the last value to -1\n",
    "            updated_q_values = updated_q_values * (1 - done_sample) - done_sample\n",
    "\n",
    "            # Create a mask so we only calculate loss on the updated Q-values\n",
    "            masks = tf.one_hot(action_sample, num_actions)\n",
    "\n",
    "            with tf.GradientTape() as tape:\n",
    "                # Train the model on the states and updated Q-values\n",
    "                q_values = model(state_sample)\n",
    "\n",
    "                # Apply the masks to the Q-values to get the Q-value for action taken\n",
    "                q_action = tf.reduce_sum(tf.multiply(q_values, masks), axis=1)\n",
    "                # Calculate loss between new Q-value and old Q-value\n",
    "                loss = loss_function(updated_q_values, q_action)\n",
    "\n",
    "            # Backpropagation\n",
    "            grads = tape.gradient(loss, model.trainable_variables)\n",
    "            optimizer.apply_gradients(zip(grads, model.trainable_variables))\n",
    "\n",
    "        if frame_count % update_target_network == 0:\n",
    "            # update the the target network with new weights\n",
    "            model_target.set_weights(model.get_weights())\n",
    "            # Log details\n",
    "            template = \"running reward: {:.2f} at episode {}, frame count {}\"\n",
    "            print(template.format(running_reward, episode_count, frame_count))\n",
    "\n",
    "        # Limit the state and reward history\n",
    "        if len(rewards_history) > max_memory_length:\n",
    "            del rewards_history[:1]\n",
    "            del state_history[:1]\n",
    "            del state_next_history[:1]\n",
    "            del action_history[:1]\n",
    "            del done_history[:1]\n",
    "\n",
    "        if done:\n",
    "            break\n",
    "\n",
    "    # Update running reward to check condition for solving\n",
    "    episode_reward_history.append(episode_reward)\n",
    "    if len(episode_reward_history) > 100:\n",
    "        del episode_reward_history[:1]\n",
    "    running_reward = np.mean(episode_reward_history)\n",
    "\n",
    "    episode_count += 1\n",
    "    cumulative_reward += [running_reward]\n",
    "    cumulative_crashes += [running_crashes]\n",
    "    episodes += [episode_count]\n",
    "\n",
    "    if running_reward > 40:  # Condition to consider the task solved\n",
    "        print(\"Solved at episode {}!\".format(episode_count))\n",
    "        break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "2bd327d0-3255-4bcf-a74d-81c54853ddad",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0\n"
     ]
    }
   ],
   "source": [
    "print(running_crashes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "aefbf4dd-a339-4e13-b2fb-8d89d2e11cca",
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "x and y must have same first dimension, but have shapes (1000,) and (1,)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[0;32m/var/folders/xp/gcdxfqps0qgggnf2tqlhk2vh0000gn/T/ipykernel_55919/3250555373.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0mplt\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtitle\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'Cumulative Rewards'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m \u001b[0mplt\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mplot\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mepisodes\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcumulative_crashes\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlabel\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'DQN for Platooning'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      3\u001b[0m \u001b[0mplt\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mxticks\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfontsize\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m6\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0mplt\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mxlabel\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'Episodes'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0mplt\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mylabel\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'Cumulative rewards'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/opt/anaconda3/envs/rlsc/lib/python3.7/site-packages/matplotlib/pyplot.py\u001b[0m in \u001b[0;36mplot\u001b[0;34m(scalex, scaley, data, *args, **kwargs)\u001b[0m\n\u001b[1;32m   3019\u001b[0m     return gca().plot(\n\u001b[1;32m   3020\u001b[0m         \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mscalex\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mscalex\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mscaley\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mscaley\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 3021\u001b[0;31m         **({\"data\": data} if data is not None else {}), **kwargs)\n\u001b[0m\u001b[1;32m   3022\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   3023\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/opt/anaconda3/envs/rlsc/lib/python3.7/site-packages/matplotlib/axes/_axes.py\u001b[0m in \u001b[0;36mplot\u001b[0;34m(self, scalex, scaley, data, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1603\u001b[0m         \"\"\"\n\u001b[1;32m   1604\u001b[0m         \u001b[0mkwargs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcbook\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnormalize_kwargs\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmlines\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mLine2D\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1605\u001b[0;31m         \u001b[0mlines\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_get_lines\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdata\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1606\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mline\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mlines\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1607\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0madd_line\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mline\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/opt/anaconda3/envs/rlsc/lib/python3.7/site-packages/matplotlib/axes/_base.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, data, *args, **kwargs)\u001b[0m\n\u001b[1;32m    313\u001b[0m                 \u001b[0mthis\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0margs\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    314\u001b[0m                 \u001b[0margs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0margs\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 315\u001b[0;31m             \u001b[0;32myield\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_plot_args\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mthis\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    316\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    317\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mget_next_color\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/opt/anaconda3/envs/rlsc/lib/python3.7/site-packages/matplotlib/axes/_base.py\u001b[0m in \u001b[0;36m_plot_args\u001b[0;34m(self, tup, kwargs, return_kwargs)\u001b[0m\n\u001b[1;32m    499\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    500\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mx\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m!=\u001b[0m \u001b[0my\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 501\u001b[0;31m             raise ValueError(f\"x and y must have same first dimension, but \"\n\u001b[0m\u001b[1;32m    502\u001b[0m                              f\"have shapes {x.shape} and {y.shape}\")\n\u001b[1;32m    503\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mx\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mndim\u001b[0m \u001b[0;34m>\u001b[0m \u001b[0;36m2\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0my\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mndim\u001b[0m \u001b[0;34m>\u001b[0m \u001b[0;36m2\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mValueError\u001b[0m: x and y must have same first dimension, but have shapes (1000,) and (1,)"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.title('Cumulative Rewards')\n",
    "plt.plot(episodes, cumulative_crashes, label='DQN for Platooning')\n",
    "plt.xticks(fontsize=6)\n",
    "plt.xlabel('Episodes')\n",
    "plt.ylabel('Cumulative rewards')\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "8cd4972d-31d1-41a6-bd58-955f8e1c1bf8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[15 14  0 -2]\n"
     ]
    }
   ],
   "source": [
    "print(state)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0332d069-a2ec-4880-90f8-ae47130b0197",
   "metadata": {},
   "source": [
    "*Interesting Note:* Even though it's punished if vehicle 2 gets in front of vehicle 1, the RL still chooses to go in the negative index. 10 - x2 + x1. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "85d3a38f-7c34-43e2-bdcf-a63e5a283ee7",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
