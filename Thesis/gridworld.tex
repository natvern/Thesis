\section{Dynamic Obstacles in a GridWorld} 
We first evaluate the performance of our dio/rl implementation compared to an implementation making only use of rl. 
\subsection{Scenario in Reinforcement Learning}
Our autonomous agent exists in an 8x8 grid world. Its goal is to reach the goal from his initial position (1,1).
Along the way, there exists dynamic obstacles which movements is unknown. The agent is punished if colliding with an obstacle and the episode, hereby ends. 
This environment offered by gym-gridworld \cite{gym_minigrid} is useful for testing our algorithm in a Dynamic Obstacle avoidance for a partially observable 
environment. Precisely, we define the state as follows. 
\begin{equation*}
  S_t = [x, y, d, G]
\end{equation*}
$(x,y)$ define the position of our agent while $d$ its direction. $G$ is the gridworld observed by the agent which includes walls, obstacles and free squares. 
The action space is, 
\begin{equation*}
  A_t = \{ right: 0, up: 1, down: 2, left: 3 \}
\end{equation*}
Finally, the reward is a function of the distance from the goal defined as, 
\begin{equation*}
  R_t = 1 - 0.9*(\dfrac{steps}{max\_steps})
\end{equation*}

\begin{multicols}{2}[\medskip]
    \begin{figure}[H]
      \centering
      \includegraphics[scale=0.55]{figures/gridworldrl.png}
      \caption{Gridworld with Dynamic Obstacles}
      \label{fig:gridrl}
    \end{figure}
    \columnbreak
    The Reinforcement Learning experiments have been performed on 1M frames for a similar start configuration as shown in \ref{fig:gridrl}. The episode ends when the agent 
    reaches the goal OR collides with an obstacle. We want to encourage the shortest and safest path, thus, the punishment for crashing is $r = -1$. Our rewards are normalized as shown in
    the reward function. We define the range of rewards to be $(-1,1)$. 
\end{multicols}

\subsection{Domain Specific Rules}
\textcolor{red}{Fix the rules.}
The rules are defined as a ProbLog \cite{problog}: a probabilistic prolog that allows us to capture 
the stochasticity of the environment. Precisely, we want to consider the erratic movements of the obstacles, considering 
we do not have previous knowledge on the distribution of their given movement. We assume a uniform distribution and define the following. 
The rules of DIO take the following form: 

\begin{prooftree}
  \AxiomC{$p_1 \dots p_n$}
  \AxiomC{$\sum_{i=0}^{n}P(i) = 1$}
  \RightLabel{(action)}
  \BinaryInfC{$P_i :: \varphi(i)$}
\end{prooftree}
We define $\sum_{i=0}^{n}P(i) = 1$, and $\varphi(i)$ corresponds to the conjunction of grounds facts of the possible world with probability $P_i$.
The action is equivalent to our step semantics, thus, we enforce that a given action modifies the facts in some form. In practice, an action is the missing 
clause to generate the next predicate. In the gridworld example, we give the following. 

\begin{multicols}{2}
\begin{prooftree}
  \AxiomC{atPos(X + V*T, Y)}
  \RightLabel{(right)}
  \LeftLabel{(1)}
  \UnaryInfC{atPos(X,Y), speed(V), timestep(T)}
\end{prooftree}
\columnbreak 
\begin{prooftree}
  \AxiomC{0.25 :: obs(X + V*T, Y, V) \ldots}
  \LeftLabel{(2)}
  \RightLabel{(time)}
  \UnaryInfC{obs(X,Y,V), timestep(T)}
\end{prooftree}
\end{multicols}

(1) considers the movement of the agent while (2) considers the movement of the obstacles. Note that (2) considers 
a uniform distribution over the movement of the obstacle, since every obstacle has a uniform probability of moving up/down/left/right. 
We could do the same for (1) by consider the probability of an action failing. In our case, we assume the movement is deterministic and no failure over the movement 
of the agent happens.


\subsection{World Knowledge}
Our world knowledge base covers the agent, the obstacles and the timestep. We consider two cases: 
\textit{constant} ground facts vs. \textit{dynamic} ground facts. The latter represents positions which are dynamically 
generated at every timestep while the former considers only the facts that remain true in every world, thus include the timestep, since we always
move by 1-unit, and the speed, since the agent and the obtacles are defined to only move by 1-box every time. Given that our knowledge base $Kb$ is defined by, 
\[
    C = \{speed(1), timestep(1) \}     
    \qquad
    D = \{atPos(X, Y), obs(X,Y,1)\}
    \qquad
    Kb = C \cup D
\]

\subsection{From Norms to Labels}
In the following, we define \textbf{\glsplural{norms}} as textual sentences to describe the intended goal behavior. 
Thus, in the gridworld example, we consider three possibles norms, (1) \textit{crash} when the agent and the obstacle overlap, (2) \textit{maybecrash} 
when the agent and the obstacle are one square from each other and finally (3) \textit{safe} when there is ample distance between the agent the obstacle 
in this case, two square distances. Those norms are identified as 'labels', i.e. predicates defined as follows. 

\[
    \infer{1 :: crash}{1 :: atPos(X,Y) & 1 :: obs(X,Y,\_)} 
    \qquad
    \infer{P_1 \times P_2 :: maybecrash}{P_1::atPos(X,Y), P_2::obs(X,Y,\_)
      & \ldots}
\] 
\[
    \infer{P_1 \times P_2 :: safe}{
      P_1::atPos(X_1,Y_1), P_2::obs(X_2,Y_2,\_),X_1 \neq X_2, Y_1 \neq Y_2
      & \ldots
    }
\]
\[
  \infer{1.0/(L+T) :: close}{
    nextPos(X,Y), goal(I,J), time(T), L = abs(I-X) + abs(Y-J)
  }
\]
 

\subsection{Translation Unit}

  \begin{algorithm}[H]
    \caption{Dio/RL Loop}
    \begin{algorithmic}[1]
    
    \Procedure{Step}{$S_t, a$}       \Comment{(State, action) Pair}
        \State Check for invalid actions
        \State Check for obstacles 
        \State Update Obstacles positions
        \State Update Agent's position
        \State \textbf{UpdateWorld}(position, direction, obstacles) \Comment{KB update in DIO}
        \State $obs, reward$ = Step'$(S_t, ac)$ \Comment{Call to Initial Env}
        \State $reward'$ = \textbf{getFeedback()} \Comment{Query DIO}
        \If{$reward' != -1$} 
            \State $reward$ += reward'
        \EndIf
        \State return (obs,reward)
    \EndProcedure
    
    \end{algorithmic}
    \end{algorithm}

\begin{algorithm}[H]
  \caption{Reward Shaping from RL to DIO/TU}
  \begin{algorithmic}[1]
      
      \Procedure{getFeedback}{} 
      \State labels $\gets$ \textbf{getLabels()} \Comment{Evaluated predicates w/ Corresponding Probabilities}
      \State $P \gets \{$crash, maybecrash, safe$\}$  \Comment{Set of possible worlds}
      \State $F \gets \{-1,-0.5,1\}$ \Comment{Numerical value associated with world}
      \State $i \gets 1$
      \State $r \gets 0$
      \While{$i \leq$ length(labels)} 
         \State $r \gets r +$ labels[P[i]]*F[i] \Comment{Equivalent to weighted probabilities}
         \State $i \gets i+1$


      \EndWhile
      
      \EndProcedure
      
      \end{algorithmic}
      \end{algorithm}

    \subsection{Preliminary Results}
    Our testing was done on a Dynamic Obstacles grid world using proximal policy optimization over 80k frames. 
    We pick up the frames per second to evaluate the speed of learning, the mean number of frames per episode 
    to evaluate the optimality of the solution and finally, the mean returns per episode to evaluate convergence relative 
    to the number of frames. 

    \begin{figure}[H]
      \centering
      \begin{minipage}{.5\textwidth}
        \centering
        \includegraphics[width=1\linewidth]{figures/diofps.png}
        \captionof{figure}{Frames per Second with Dio}
        \label{fig:fpsdio}
      \end{minipage}%
      \begin{minipage}{.5\textwidth}
        \centering
        \includegraphics[width=1\linewidth]{figures/rlfps.png}
        \captionof{figure}{Frames per Second without Dio}
        \label{fig:fpsrl}
      \end{minipage}
    \end{figure}

    First, as expected, the implementation using Dio is much slower only able to run 
    an average of 150 episodes per second. As a result, for 80k episodes, the implementation with Dio 
    took 9 minutes to train versus 16 seconds for the implementation without. However, this loss in time shows as in \ref{fig:framesdio} 
    a gain in optimality. When visualizing the learned policy of Dio, we found that our agent succeeds in \textcolor{red}{percentage of success} 
    of the time to reach the goal without colliding while the policy learned without Dio did not reach a suitable solution and fails in \textcolor{red}{add percentage} 
    of the time. 

    \begin{figure}[H]
      \centering
      \begin{minipage}{.5\textwidth}
        \centering
        \includegraphics[width=1\linewidth]{figures/dioframesmean.png}
        \captionof{figure}{Mean of frames per episode with Dio}
        \label{fig:framesdio}
      \end{minipage}%
      \begin{minipage}{.5\textwidth}
        \centering
        \includegraphics[width=1\linewidth]{figures/rlframesmean.png}
        \captionof{figure}{Mean of frames per episode without Dio}
        \label{fig:framesrl}
      \end{minipage}
    \end{figure}

    The dip in the mean frames in \ref{fig:framesdio} show that the agent did not only learn 
    how to only collide but also is learning how to reach the goal in the shortest amount of time. 
    Since \ref{fig:returndio} is still increasing, we can also tell that the values did not converge yet and 
    the agent can learn even a better optimization while \ref{fig:returnrl} shows that the values converge towards
    a suboptimal policy.

    \begin{figure}[H]
      \centering
      \begin{minipage}{.5\textwidth}
        \centering
        \includegraphics[width=1\linewidth]{figures/dioreturn.png}
        \captionof{figure}{Mean of returns per episode with Dio}
        \label{fig:returndio}
      \end{minipage}%
      \begin{minipage}{.5\textwidth}
        \centering
        \includegraphics[width=1\linewidth]{figures/rlreturn.png}
        \captionof{figure}{Mean of returns per episode without Dio}
        \label{fig:returnrl}
      \end{minipage}
    \end{figure}

    Overall, the results show that an implementation that makes use of dio is beneficial even in a stochastic environment. 
    As we lose in speed, we gain in optimality of the solution. 
    Given those preliminary results, we want to (1) optimize our implementation 
    and (2) test our implementation in a more complex scenario that requires a more complex understanding of the world 
    from dio's side. (1) is discussed in \ref{optimality} and (2) is discussed in \ref{traffic}. 

