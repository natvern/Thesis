{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "a01abb11-8a0d-4fcd-9605-badd0b28e7d7",
   "metadata": {},
   "outputs": [],
   "source": [
    "from optparse import OptionParser\n",
    "\n",
    "import random\n",
    "import pandas\n",
    "import numpy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "2d2e4cd0-f19e-4e10-a504-a8c020944c55",
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "137c4fdf-5b8c-49ff-a762-1fc1e02a6ec6",
   "metadata": {},
   "outputs": [],
   "source": [
    "import gym\n",
    "import gym_hw"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "a85848fc-6646-4051-b4b4-08c4eabada00",
   "metadata": {},
   "outputs": [],
   "source": [
    "env = gym.make('gridworld_hw-v0')\n",
    "env.verbose = False # no graphics\n",
    "LIFETIME = 10**7\n",
    "num_states = 6\n",
    "num_actions = 4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "e620661f-21cb-44a8-bf8e-68e4f6728ec3",
   "metadata": {},
   "outputs": [],
   "source": [
    "def observation_to_state(obs):\n",
    "    return obs\n",
    "def fromcolrow(r, c):\n",
    "    if r == 0 and c == 0:\n",
    "        return 0\n",
    "    if r == 0 and c == 1:\n",
    "        return 1\n",
    "    if r == 1 and c == 0:\n",
    "        return 2 \n",
    "    if r == 1 and c == 1:\n",
    "        return 3\n",
    "    if r == 2 and c == 0:\n",
    "        return 4\n",
    "    if r == 2 and c == 1:\n",
    "        return 5\n",
    "    return None\n",
    "def convert(a):\n",
    "    return a"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f3b64838-7543-4eca-bcf0-ba79afb9ca41",
   "metadata": {},
   "source": [
    "# Review equivalent to a safe controller action choice"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "be751940-4e3e-4102-a3f2-a847720aeb60",
   "metadata": {},
   "outputs": [],
   "source": [
    "def review(state, a): \n",
    "    if state == 2 and a == 1: \n",
    "        return False \n",
    "    return True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "cce22c11-4655-46e0-9aa7-4a1df03d4f70",
   "metadata": {},
   "outputs": [],
   "source": [
    "def epsilon_greedy(S, Q, epsilon):\n",
    "    if numpy.random.random() < epsilon:\n",
    "        return numpy.random.choice(num_actions)\n",
    "    return numpy.argmax(Q[S])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "51cc7323-b173-47a4-9485-cb5f0ba37013",
   "metadata": {},
   "outputs": [],
   "source": [
    "def q_learning(env, obs_to_state, alpha, gamma, epsilon, initial_q, lifetime=LIFETIME):\n",
    "    Q = numpy.full((num_states, num_actions), float(initial_q)) # Initialize Q(s,a)\n",
    "    t = 0\n",
    "    observation = env.reset()\n",
    "    G = initializeKb()\n",
    "    total_reward = 0 # Initialize total reward\n",
    "    state = obs_to_state(observation) # Initiliaze S\n",
    "    while (t < lifetime): # Stopping criterion\n",
    "        a = epsilon_greedy(state, Q, epsilon)\n",
    "        #if (not review(state, a)):\n",
    "        #    continue\n",
    "        if (believesBad(G, state, a)):\n",
    "            continue\n",
    "        N = env.step(a)\n",
    "        new_state = obs_to_state(N[0])\n",
    "        reward = N[1]\n",
    "        total_reward += reward\n",
    "        ## Calculating maxa Q(S', a)\n",
    "        maxQ = -float(\"inf\")\n",
    "        for action in range(num_actions):\n",
    "            if Q[new_state, action] > maxQ:\n",
    "                maxQ = Q[new_state, action]\n",
    "        Q[state, a] = Q[state, a] + alpha * (reward + gamma*maxQ - Q[state, a])\n",
    "        state = new_state\n",
    "        t += 1 # lost one life \n",
    "    return total_reward, Q"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "76b0394f-62da-4e0d-a874-15121e203631",
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_policy(Q, algo, pi=None):\n",
    "    rows = 3\n",
    "    cols = 2\n",
    "    pi_arr = numpy.full((rows, cols), 0)\n",
    "    for r in range(rows):\n",
    "        for c in range(cols):\n",
    "            Qmax = -float(\"inf\")\n",
    "            Amax = None\n",
    "            state = fromcolrow(r, c)\n",
    "            for a in range(num_actions):\n",
    "                if Q[state, a] > Qmax:\n",
    "                    Qmax = Q[state, a]\n",
    "                    Amax = convert(a)\n",
    "            pi_arr[r, c] = Amax\n",
    "    f, _ = plt.subplots()\n",
    "    plt.pcolormesh(pi_arr, cmap=plt.cm.get_cmap('viridis', 4))\n",
    "    plt.colorbar(ticks=range(4), label=\"action\")\n",
    "    plt.xticks(range(cols))\n",
    "    plt.yticks(range(rows))\n",
    "    plt.title(algo + \" Policy\")\n",
    "    plt.savefig(\"policy.png\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "809acec8-ea61-4a3e-8dd6-6ca427a58c8f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_cumulative_reward(env, obs_to_state):\n",
    "    exps = [(\"Q-Learning\",0.1,0.5,0.2,0)]\n",
    "    fig = plt.figure()\n",
    "    ax = plt.axes()\n",
    "    plt.title(\"Cumulative Reward given Lifetime\")\n",
    "    plt.xlabel(\"Lifetime\")\n",
    "    plt.ylabel(\"Cumulative Reward\")\n",
    "    for exp in exps:\n",
    "        alpha = exp[1]\n",
    "        gamma = exp[2]\n",
    "        epsilon = exp[3]\n",
    "        initial_q = exp[4]\n",
    "        x = []\n",
    "        for i in range(19):\n",
    "            x += [2**i]\n",
    "        y = []\n",
    "        for life in x:\n",
    "            r, Q = q_learning(env, obs_to_state, alpha, gamma, epsilon, initial_q, life)\n",
    "            y += [r]\n",
    "        plt.plot(x, y, label = exp[0] + \"- alpha: \" + str(alpha) + \" gamma: \" + str(gamma) + \" epsilon: \"+ str(epsilon))\n",
    "    #plot_policy(Q, \"Q-Learning with gamma = 0.1\")\n",
    "    plt.legend(loc=\"lower right\")\n",
    "    fig.savefig(\"cumulative+belief.png\", facecolor=\"white\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "53513e77-2b10-4a85-952a-b3fd4c1f1254",
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_cumulative_reward(env, observation_to_state)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "94cb9b90-df2b-4db5-bd78-216b12a3df23",
   "metadata": {},
   "source": [
    "Note that when adding a review, the algorithm converges faster [evident in the comparison between the cumulative reward of both implementation]. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a818dc13-b861-4a31-b182-032fc33e204b",
   "metadata": {},
   "source": [
    "# Knowledge Base as History"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "0526b38d-9f0e-4412-9bbe-82784e268067",
   "metadata": {},
   "outputs": [],
   "source": [
    "def initializeKb():\n",
    "    return set()\n",
    "def kb(G, s0, a, r):\n",
    "    G.add((s0, a, r)) \n",
    "    return G \n",
    "def believesBad(G, s, a):\n",
    "    knows = [] \n",
    "    for p in G:\n",
    "        if p[0] == s and p[1] == a:\n",
    "            knows += [r] \n",
    "    if sum(knows) < 0:\n",
    "        return True\n",
    "    return False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5ae8f2ad-5184-4546-8970-981bfde1b251",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
