@misc{riegel2020logical,
      title={Logical Neural Networks}, 
      author={Ryan Riegel and Alexander Gray and Francois Luus and Naweed Khan and Ndivhuwo Makondo and Ismail Yunus Akhalwaya and Haifeng Qian and Ronald Fagin and Francisco Barahona and Udit Sharma and Shajith Ikbal and Hima Karanam and Sumit Neelam and Ankita Likhyani and Santosh Srivastava},
      year={2020},
      eprint={2006.13155},
      archivePrefix={arXiv},
      primaryClass={cs.AI}
}

@misc{garcez2020neurosymbolic,
      title={Neurosymbolic AI: The 3rd Wave}, 
      author={Artur d'Avila Garcez and Luis C. Lamb},
      year={2020},
      eprint={2012.05876},
      archivePrefix={arXiv},
      primaryClass={cs.AI}
}

@INPROCEEDINGS{vnc20, 
      author={Dantas, Yuri Gil and Nigam, Vivek and Talcott, Carolyn},  
      booktitle={2020 IEEE Vehicular Networking Conference (VNC)},   
      title={A Formal Security Assessment Framework for Cooperative Adaptive Cruise Control},   
      year={2020},  
      volume={},  
      number={},  
      pages={1-8},  
      doi={10.1109/VNC51378.2020.9318334}}

@article{Sarker2021NeuroSymbolicAI,
  title={Neuro-Symbolic Artificial Intelligence: Current Trends},
  author={Md. Kamruzzaman Sarker and Lu Zhou and Aaron Eberhart and P. Hitzler},
  journal={ArXiv},
  year={2021},
  volume={abs/2105.05330}
}

@article{schweighofer_meta-learning_2003,
	title = {Meta-learning in {Reinforcement} {Learning}},
	volume = {16},
	OPTurl = {https://linkinghub.elsevier.com/retrieve/pii/S0893608002002289},
	OPTdoi = {10.1016/S0893-6080(02)00228-9},
	abstract = {Meta-parameters in reinforcement learning should be tuned to the environmental dynamics and the animal performance. Here, we propose a biologically plausible meta-reinforcement learning algorithm for tuning these meta-parameters in a dynamic, adaptive manner. We tested our algorithm in both a simulation of a Markov decision task and in a non-linear control task. Our results show that the algorithm robustly ﬁnds appropriate meta-parameter values, and controls the meta-parameter time course, in both static and dynamic environments. We suggest that the phasic and tonic components of dopamine neuron ﬁring can encode the signal required for meta-learning of reinforcement learning. q 2002 Elsevier Science Ltd. All rights reserved.},
	language = {en},
	number = {1},
	journal = {Neural Networks},
	author = {Schweighofer, Nicolas and Doya, Kenji},
	month = jan,
	year = {2003},
	pages = {5--9},
}

@article{Garca2015ACS,
  title={A comprehensive survey on safe reinforcement learning},
  author={Javier Garc{\'i}a and F. Fern{\'a}ndez},
  journal={J. Mach. Learn. Res.},
  year={2015},
  volume={16},
  pages={1437-1480}
}

@article{law2005,
author = {Law, Edith},
year = {2005},
month = {01},
pages = {},
title = {Risk-directed exploration in reinforcement learning}
}

@inproceedings{gupta_meta-reinforcement_2018,
	title = {Meta-{Reinforcement} {Learning} of {Structured} {Exploration} {Strategies}},
	abstract = {Exploration is a fundamental challenge in reinforcement learning (RL). Many current exploration methods for deep RL use task-agnostic objectives, such as information gain or bonuses based on state visitation. However, many practical applications of RL involve learning more than a single task, and prior tasks can be used to inform how exploration should be performed in new tasks. In this work, we study how prior tasks can inform an agent about how to explore effectively in new situations. We introduce a novel gradient-based fast adaptation algorithm – model agnostic exploration with structured noise (MAESN) – to learn exploration strategies from prior experience. The prior experience is used both to initialize a policy and to acquire a latent exploration space that can inject structured stochasticity into a policy, producing exploration strategies that are informed by prior knowledge and are more effective than random action-space noise. We show that MAESN is more effective at learning exploration strategies when compared to prior meta-RL methods, RL without learned exploration strategies, and task-agnostic exploration methods. We evaluate our method on a variety of simulated tasks: locomotion with a wheeled robot, locomotion with a quadrupedal walker, and object manipulation.},
	language = {en},
	booktitle = {Conference and {Workshop} on {Neural} {Information} {Processing} {Systems} ({NeurIPS})},
	author = {Gupta, Abhishek and Mendonca, Russell and Liu, YuXuan and Abbeel, Pieter and Levine, Sergey},
	year = {2018},
	pages = {10},
}


@article{Besold2017NeuralSymbolicLA,
  title={Neural-Symbolic Learning and Reasoning: A Survey and Interpretation},
  author={Tarek R. Besold and A. Garcez and Sebastian Bader and H. Bowman and Pedro M. Domingos and P. Hitzler and Kai-Uwe K{\"u}hnberger and L. Lamb and Daniel Lowd and P. Lima and L. Penning and Gadi Pinkas and Hoifung Poon and Gerson Zaverucha},
  journal={ArXiv},
  year={2017},
  volume={abs/1711.03902}
}

@inproceedings{vinyals2015,
author = {Vinyals, Oriol and Toshev, Alexander and Bengio, Samy and Erhan, Dumitru},
year = {2015},
month = {06},
pages = {3156-3164},
title = {Show and tell: A neural image caption generator},
doi = {10.1109/CVPR.2015.7298935}
}

@inproceedings{karpathy2015,
author = {Karpathy, Andrej and Li, Fei},
year = {2015},
month = {06},
pages = {3128-3137},
title = {Deep visual-semantic alignments for generating image descriptions},
doi = {10.1109/CVPR.2015.7298932}
}

@inproceedings{Reed2015DeepVA,
  title={Deep Visual Analogy-Making},
  author={Scott E. Reed and Yi Zhang and Y. Zhang and Honglak Lee},
  booktitle={NIPS},
  year={2015}
}

@article{Foerster2016LearningTC,
  title={Learning to Communicate to Solve Riddles with Deep Distributed Recurrent Q-Networks},
  author={Jakob N. Foerster and Yannis M. Assael and N. D. Freitas and S. Whiteson},
  journal={ArXiv},
  year={2016},
  volume={abs/1602.02672}
}

@techreport{li2019reinforcement,
	title = {Reinforcement {Learning} {Applications}},
	url = {http://arxiv.org/abs/1908.06973},
	abstract = {We start with a brief introduction to reinforcement learning (RL), about its successful stories, basics, an example, issues, the ICML 2019 Workshop on RL for Real Life, how to use it, study material and an outlook. Then we discuss a selection of RL applications, including recommender systems, computer systems, energy, finance, healthcare, robotics, and transportation.},
	number = {arXiv:1908.06973},
	author = {Li, Yuxi},
	month = aug,
	year = {2019},
	keywords = {Computer Science - Artificial Intelligence, Computer Science - Machine Learning},
	file = {Li - 2019 - Reinforcement Learning Applications.pdf:/Users/giannidicaro/Zotero/storage/4Y4W8K55/Li - 2019 - Reinforcement Learning Applications.pdf:application/pdf}
}

@article{rockafellar2000,
author = {Rockafellar, R and Uryasev, Stan},
year = {2000},
month = {01},
pages = {21-42},
title = {Optimization of Conditional Value-At-Risk},
volume = {2},
journal = {Journal of risk}
}

@article{Siebel2007EvolutionaryRL,
  title={Evolutionary reinforcement learning of artificial neural networks},
  author={Nils T. Siebel and G. Sommer},
  journal={Int. J. Hybrid Intell. Syst.},
  year={2007},
  volume={4},
  pages={171-183}
}

@inproceedings{Li2021SafeRL,
  title={Safe Reinforcement Learning Using Robust Action Governor},
  author={Yutong Li and N. Li and H. E. Tseng and A. Girard and Dimitar Filev and I. Kolmanovsky},
  booktitle={L4DC},
  year={2021}
}

@article{alshiekh2017,
author = {Alshiekh, Mohammed and Bloem, Roderick and Ehlers, Rüdiger and Könighofer, Bettina and Niekum, Scott and Topcu, Ufuk},
year = {2017},
month = {08},
pages = {},
title = {Safe Reinforcement Learning via Shielding}
}